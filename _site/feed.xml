<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-09-09T16:20:06+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">오류동 개발자</title><subtitle>개발과 일상적인 이야기들</subtitle><author><name>김동혁</name><email>dhkdn9192@naver.com</email></author><entry><title type="html">PySpark의 py4j 호환성 오류</title><link href="http://localhost:4000/dev/pyspark-py4j-error/" rel="alternate" type="text/html" title="PySpark의 py4j 호환성 오류" /><published>2020-09-09T00:00:00+09:00</published><updated>2020-09-09T00:00:00+09:00</updated><id>http://localhost:4000/dev/pyspark-py4j-error</id><content type="html" xml:base="http://localhost:4000/dev/pyspark-py4j-error/">&lt;h2 id=&quot;1-py4jerror&quot;&gt;1. Py4JError&lt;/h2&gt;
&lt;p&gt;기존에 사용하던 Spark을 2.2.0 -&amp;gt; 2.4.6으로 변경했더니 Jupyter에서 PySpark이 정상적으로 동작하지 않는 오류가 발생했다.
실행한 코드는 아래와 같이 SparkSession을 생성하는 간단한 내용이다.&lt;/p&gt;

&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pyspark&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;sql&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;SparkSession&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;SparkSession&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;builder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;getOrCreate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;SparkSession을 생성하려고 하면 다음과 같이 Py4JError가 발생한다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Py4JError                                 Traceback (most recent call last)
&amp;lt;ipython-input-3-ce55329f3d67&amp;gt; in &amp;lt;module&amp;gt;()
----&amp;gt; 1 spark = SparkSession.builder.getOrCreate()
      2 spark

/opt/spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/session.py in getOrCreate(self)
/opt/spark-2.4.6-bin-hadoop2.7/python/pyspark/context.py in getOrCreate(cls, conf)
/opt/spark-2.4.6-bin-hadoop2.7/python/pyspark/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)
/opt/spark-2.4.6-bin-hadoop2.7/python/pyspark/context.py in _ensure_initialized(cls, instance, gateway, conf)
/opt/spark-2.4.6-bin-hadoop2.7/python/pyspark/java_gateway.py in launch_gateway(conf)
/opt/spark-2.4.6-bin-hadoop2.7/python/pyspark/java_gateway.py in _launch_gateway(conf, insecure)
/data/venv/lib64/python3.6/site-packages/py4j/java_gateway.py in java_import(jvm_view, import_str)
/data/venv/lib64/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    321                 raise Py4JError(
    322                     &quot;An error occurred while calling {0}{1}{2}. Trace:\n{3}\n&quot;.
--&amp;gt; 323                     format(target_id, &quot;.&quot;, name, value))
    324         else:
    325             raise Py4JError(

Py4JError: An error occurred while calling None.None. Trace:
Authentication error: unexpected command. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-pyspark의-jvm-연동&quot;&gt;2. PySpark의 JVM 연동&lt;/h2&gt;
&lt;p&gt;위 현상은 새로 설치한 spark의 pyspark shell에서는 발생하지 않는다.
기존에 사용하던 python 가상환경에서 pyspark을 import하여 사용할 때에만 발생한다.&lt;/p&gt;

&lt;p&gt;PySpark은 python으로 작성된 코드를 jvm에서 수행하기 위해 py4j 라이브러리를 사용한다.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${SPARK_HOME}/python/pyspark/java_gateway.py&lt;/code&gt;를 보면 
py4j의 java_import 함수를 사용하여 필요한 클래스들을 jvm에 import한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;c1&quot;&gt;# Import the classes used by PySpark
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;java_import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jvm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark.SparkConf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;java_import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jvm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark.api.java.*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;java_import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jvm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark.api.python.*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;java_import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jvm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark.ml.python.*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;java_import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jvm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark.mllib.api.python.*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# TODO(davies): move into sql
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;java_import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jvm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark.sql.*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;java_import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jvm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark.sql.api.python.*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;java_import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jvm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark.sql.hive.*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;java_import&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gateway&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jvm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;scala.Tuple2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-원인과-해결책&quot;&gt;3. 원인과 해결책&lt;/h2&gt;

&lt;p&gt;여기서 오류가 발생한 이유는 정말 사소한 것 때문이었다.
기존에 사용하던 PySpark을 위해 py4j를 설치했었는데,
신규 설치한 Spark이 요구하는 버전과 달라서 오류가 발생했던 것이다.&lt;/p&gt;

&lt;p&gt;기존 사용하던 python 가상환경에는 py4j 0.10.4 버전이 설치되어 있었다.
반면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${SPARK_HOME}/python/setup.py&lt;/code&gt;에서 신규 설치한 pyspark이 요구하는 py4j 버전은 다음과 같다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;install_requires&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'py4j==0.10.7'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;요구하는 버전을 설치하여 정말 간단하게 해결할 수 있는 문제였다.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;py4j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.10.7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>김동혁</name><email>dhkdn9192@naver.com</email></author><category term="dev" /><category term="spark" /><category term="pyspark" /><category term="py4j" /><summary type="html">1. Py4JError 기존에 사용하던 Spark을 2.2.0 -&amp;gt; 2.4.6으로 변경했더니 Jupyter에서 PySpark이 정상적으로 동작하지 않는 오류가 발생했다. 실행한 코드는 아래와 같이 SparkSession을 생성하는 간단한 내용이다.</summary></entry><entry><title type="html">HDP 2.6에서 Spark 업그레이드하기</title><link href="http://localhost:4000/dev/hdp-spark-upgrade/" rel="alternate" type="text/html" title="HDP 2.6에서 Spark 업그레이드하기" /><published>2020-09-08T00:00:00+09:00</published><updated>2020-09-08T00:00:00+09:00</updated><id>http://localhost:4000/dev/hdp-spark-upgrade</id><content type="html" xml:base="http://localhost:4000/dev/hdp-spark-upgrade/">&lt;h2 id=&quot;1-spark-22의-import-버그&quot;&gt;1. Spark 2.2의 import 버그&lt;/h2&gt;
&lt;p&gt;현재 사용 중인 하둡 클러스터는 HDP 2.6이며 Spark은 2.2 버전을 제공하고 있다.
해당 버전은 spark-shell에서 정상적으로 import가 작동하지 않는 버그(&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-22393&quot;&gt;SPARK-22393&lt;/a&gt;)가 존재하므로 Zeppelin에서 제대로 Spark 코드를 수행할 수가 없었다.
따라서 해당 버그가 수정된 버전인 Spark 2.4 버전을 HDP 2.6에 설치해 보고자 한다.&lt;/p&gt;

&lt;h2 id=&quot;2-spark-업그레이드-하기&quot;&gt;2. Spark 업그레이드 하기&lt;/h2&gt;

&lt;h3 id=&quot;2-1-spark-24-다운로드-및-설치&quot;&gt;2-1. Spark 2.4 다운로드 및 설치&lt;/h3&gt;

&lt;p&gt;우선 Apache Ambari 관리모드에서 기존 Spark을 삭제한 뒤, Spark 신규 버전을 Master 노드에 다운로드하였다.
HDP 2.6에선 Hadoop 2.7을 사용하므로 &lt;strong&gt;Pre-built for Apache Hadoop 2.7&lt;/strong&gt;을 선택한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dhkdn9192/dhkdn9192.github.io/master/assets/images/posts/2020/09/08/2020-09-08-download-spark.png&quot; alt=&quot;download-spark&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-2-spark-envsh-설정&quot;&gt;2-2. spark-env.sh 설정&lt;/h3&gt;
&lt;p&gt;원하는 곳에 압축 해제한 뒤 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conf/spark-env.sh&lt;/code&gt; 파일을 열어 아래와 같이 수정한다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/hdp/2.6.3.0-235/hadoop
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_CONF_DIR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/hadoop/conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Spark은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${HADOOP_CONF_DIR}/yarn-site.xml&lt;/code&gt; 파일로부터 YARN과 연결하기 위한 정보들을 가져오게 된다.&lt;/p&gt;

&lt;h3 id=&quot;2-3-spark-shell-실행&quot;&gt;2-3. spark-shell 실행&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bin/spark-shell&lt;/code&gt;을 수행하면 REPL 환경에서 Spark 프로그래밍을 할 수 있다.
별도의 옵션 없이 수행하면 standalone 모드로 동작한다. YARN과 연동하여 하둡 클러스터를 이용하려면 yarn-client 모드로 동작해야 한다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./bin/spark-shell &lt;span class=&quot;nt&quot;&gt;--master&lt;/span&gt; yarn &lt;span class=&quot;nt&quot;&gt;--deploy-mode&lt;/span&gt; client
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-4-yarn-timeline-service-이슈&quot;&gt;2-4. YARN timeline-service 이슈&lt;/h3&gt;

&lt;p&gt;위와 같이 yarn-client 모드를 수행하면 다음과 같은 에러가  발생한다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig
  at org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient(TimelineClient.java:55)
  at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createTimelineClient(YarnClientImpl.java:181)
  at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:168)
  at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
  at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:161)
  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:188)
  at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:501)
  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
  at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
  ... 62 elided
Caused by: java.lang.ClassNotFoundException: com.sun.jersey.api.client.config.ClientConfig
  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 76 more
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;다행히도 이 오류는 &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-15343&quot;&gt;SPARK-15343&lt;/a&gt;에서 다뤄진 적이 있다.
이슈 내용을 정라하자면 핵심은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;YARN의 timeline-service 기능과 관련하여 jersey 라이브러리를 사용하고 있다.&lt;/li&gt;
  &lt;li&gt;기존에 사용하던 HDP 환경에선 YARN과 Spark 모두 jersey 1.9 버전을 사용하고 있었으나, &lt;strong&gt;Spark 2.4는 2.22.2 버전을 사용하므로 서로 호환되지 않는다&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;YARN은 디폴트로 timeline-service 기능이 켜져있으므로 항상 jersey 호환성으로 인한 NoClassDefFoundError를 일으키게 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;jira 이슈에서 제시되는 몇가지 해결책들이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;timeline-service를 중지하기 위해 YARN 설정에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;yarn.timeline-service.enabled&lt;/code&gt; 옵션을 false로 변경하거나&lt;/li&gt;
  &lt;li&gt;spark-shell을 수행할 때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--conf spark.timeline-service.enabled=false&lt;/code&gt; 옵션을 추가해주거나&lt;/li&gt;
  &lt;li&gt;spark의 jars 디렉토리에 jersey 1.9 라이브러리를 넣어서 버전을 통일시켜주는 방법이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;YARN에 제출되는 다양한 애플리케이션들 중 오직 Spark만을 위해서 timeline-service 기능을 끄는 것은 다소 실용성이 떨어진다.
jersey의 버전을 강제적으로 통일했을 땐 부가적인 문제가 생길 가능성이 있으므로 여기서는 두번째 방안을 채택하기로 했다.&lt;/p&gt;

&lt;h3 id=&quot;2-5-yarn-application-has-already-ended&quot;&gt;2-5. YARN application has already ended!&lt;/h3&gt;

&lt;p&gt;timeline-service 이슈를 해결했더니 이번에는 다음과 같은 에러가 발생했다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;20/09/08 19:42:53 ERROR cluster.YarnClientSchedulerBackend: The YARN application has already ended! Itmight have been killed or the Application Master may have failed to start. Check the YARN application logs for more details.
20/09/08 19:42:53 ERROR spark.SparkContext: Error initializing SparkContext.
org.apache.spark.SparkException: Application application_1599556837475_0006 failed 2 times due to AM Container for appattempt_1599556837475_0006_000002 exited with  exitCode: 1
For more detailed output, check the application tracking page: http://불라불라불라/cluster/app/application_1599556837475_0006 Then click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_e16_1599556837475_0006_02_000001
Exit code: 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;YARN에 작업이 제출된 즉시 애플리케이션이 종료된다.
원인을 찾느라 한참 헤맸는데 사실은 정말 단순한 이유였다.
HDP에선 Spark job을 제출할 때 java 옵션으로 HDP 버전을 명시해줘야 하는데,
HDP와는 별개로 설치한 Spark 패키지여서 이 부분을 생략했던 것이다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55931970/how-can-i-run-spark-in-headless-mode-in-my-custom-version-on-hdp&quot;&gt;스택오버플로우&lt;/a&gt;에 올라온 답변을 참조하여 다음과 같이 옵션을 추가하였다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./bin/spark-shell &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--master&lt;/span&gt; yarn &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--deploy-mode&lt;/span&gt; client &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--conf&lt;/span&gt; spark.hadoop.yarn.timeline-service.enabled&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--conf&lt;/span&gt; spark.driver.extraJavaOptions&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'-Dhdp.version=2.6.3.0-235'&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--conf&lt;/span&gt; spark.yarn.am.extraJavaOptions&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'-Dhdp.version=2.6.3.0-235'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;여기까지 수행한 끝에 HDP 2.6에서 Spark 2.4 버전을 사용할 수 있게 되었다.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-Dhdp.version&lt;/code&gt; 옵션은 설정 파일 등에 추가할 생각이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dhkdn9192/dhkdn9192.github.io/master/assets/images/posts/2020/09/08/2020-09-08-spark-shell-result.jpeg&quot; alt=&quot;result&quot; /&gt;&lt;/p&gt;</content><author><name>김동혁</name><email>dhkdn9192@naver.com</email></author><category term="dev" /><category term="spark" /><category term="scala" /><category term="hdp" /><category term="yarn" /><summary type="html">1. Spark 2.2의 import 버그 현재 사용 중인 하둡 클러스터는 HDP 2.6이며 Spark은 2.2 버전을 제공하고 있다. 해당 버전은 spark-shell에서 정상적으로 import가 작동하지 않는 버그(SPARK-22393)가 존재하므로 Zeppelin에서 제대로 Spark 코드를 수행할 수가 없었다. 따라서 해당 버그가 수정된 버전인 Spark 2.4 버전을 HDP 2.6에 설치해 보고자 한다.</summary></entry><entry><title type="html">Spark Shell import 에러</title><link href="http://localhost:4000/dev/sparkshell-import-error/" rel="alternate" type="text/html" title="Spark Shell import 에러" /><published>2020-09-07T00:00:00+09:00</published><updated>2020-09-07T00:00:00+09:00</updated><id>http://localhost:4000/dev/sparkshell-import-error</id><content type="html" xml:base="http://localhost:4000/dev/sparkshell-import-error/">&lt;h2 id=&quot;1-문제-발견&quot;&gt;1. 문제 발견&lt;/h2&gt;
&lt;p&gt;Zeppelin에서 Spark을 사용하다가 이해할 수 없는 에러를 보게 되었다.
아래는 “&lt;a href=&quot;https://github.com/sryza/aas/blob/master/ch08-geotime/src/main/scala/com/cloudera/datascience/geotime/RichGeometry.scala&quot;&gt;9가지 사례로 익히는 고급 스파크 분석&lt;/a&gt;” 책의 예제코드이다.&lt;/p&gt;

&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;com.esri.core.geometry.Geometry&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RichGeometry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;geometry&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Geometry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 코드를 Zeppelin에서 수행하게 되면 아래와 같은 에러가 뜬다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dhkdn9192/dhkdn9192.github.io/master/assets/images/posts/2020/09/07/2020-09-07-zeppelin-error.png&quot; alt=&quot;zeppelin-error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 줄의 java esri 라이브러리는 Zeppelin의 Spark interpreter 옵션에서 Dependencies 항목에 jar 파일 경로로 추가했다.
import 자체는 문제 없이 수행된다.
문제는 import되었음에도 Spark interpreter가 인식을 못 하는 것이다. (심지어 같은 paragraph에서 실행했다!)&lt;/p&gt;

&lt;p&gt;위 문제가 발생한 환경은 다음과 같다&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;name&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;version&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Scala&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.11.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Spark&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.2.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;2-stackoverflow-검색&quot;&gt;2. StackOverflow 검색&lt;/h2&gt;

&lt;p&gt;Stackoverflow에서 검색을 하다 크게 2가지 해결 방법을 찾았다.&lt;/p&gt;

&lt;h3 id=&quot;2-1-세미콜론으로-라인-이어붙이기&quot;&gt;2-1. 세미콜론으로 라인 이어붙이기&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;세미콜론&lt;/em&gt;을 이용하여 import 구문과 class 선언 구문을 한 라인으로 이어 붙이는 방법이다. (…)
정상적으로 수행은 가능하지만 코드가 매우 기괴해진다.&lt;/p&gt;

&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;com.esri.core.geometry.Geometry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RichGeometry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;geometry&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Geometry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,...)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-2-싱글톤으로-감싸기&quot;&gt;2-2. 싱글톤으로 감싸기&lt;/h3&gt;

&lt;p&gt;import 구문부터 class 선언구문 전체를 하나의 싱글톤으로 감싼다.
싱글톤 내부에서 import하므로 정상적으로 class 선언이 가능해진다.
그러나 클래스를 직접 호출할 수 없고 싱글톤 내부에서 가져와야 한다.&lt;/p&gt;

&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;com.esri.core.geometry.Geometry&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RichGeometry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;geometry&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Geometry&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
두 가지 모두 마음에 드는 해결책은 아니다.
구체적으로 원인이 무엇인지, 근본적인 해결책은 어떤건지 자세히 알아봐야겠다.&lt;/p&gt;

&lt;h2 id=&quot;3-근본적인-원인과-해결책&quot;&gt;3. 근본적인 원인과 해결책&lt;/h2&gt;
&lt;p&gt;Zeppelin 상에서 발견해서 당연히 Zeppelin 상의 버그인 줄 알았는데 알고보니 Spark-Shell의 오류였다.
(좀 더 깊게 들어가면 Scala까지 가게 된다.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-22393&quot;&gt;SPARK-22393&lt;/a&gt; 이슈에 따르면 Spark 2.x대에 들어서면서 발생한 버그다.
Scala 언어에서 2.11~2.12 버전 사이에 importHandler 관련 버그 픽스(&lt;a href=&quot;https://github.com/scala/bug/issues/9881&quot;&gt;SI-9880&lt;/a&gt;)가 있었는데 해당 이슈로 인해 Spark에도 import 관련 버그가 발생한 것으로 보인다.&lt;/p&gt;

&lt;p&gt;컨트리뷰터들 사이에서도 굉장히 난해한 버그였던 것 같다. 핵심만 정리하자면&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scala 2.11 에서 동작하는 Spark에서 해당 버그 발생함&lt;/li&gt;
  &lt;li&gt;Scala 2.12 에서는 importHandler 이슈가 해결됨에 따라 spark-shell 역시 정상적으로 동작함&lt;/li&gt;
  &lt;li&gt;spark-shell의 버그를 고치기 위해 Scala까지 수정하는 것은 매우 큰 작업이고 리스크가 크므로 실용적이지 않음&lt;/li&gt;
  &lt;li&gt;따라서 Scala의 fix를 적절히 이용하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SparkExprTyper&lt;/code&gt; 및 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SparkILoopInterpreter&lt;/code&gt;를 spark-shell에 추가하여 버그픽스함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, Spark 2.3부터는 해당 이슈가 해결되었다.
Spark의 버전을 업그레이드하는 것이 가장 합리적인 해결책이다.&lt;/p&gt;

&lt;p&gt;문제가 있다면 내가 사용하는 HDP에서는 Spark 버전이 2.2라는 것이다.
HDP와는 별도로 Spark 최신버전을 설치하고 Zeppelin에 연동하는 방식으로 사용해야할 것 같다.&lt;/p&gt;</content><author><name>김동혁</name><email>dhkdn9192@naver.com</email></author><category term="dev" /><category term="spark" /><category term="zeppelin" /><category term="scala" /><summary type="html">1. 문제 발견 Zeppelin에서 Spark을 사용하다가 이해할 수 없는 에러를 보게 되었다. 아래는 “9가지 사례로 익히는 고급 스파크 분석” 책의 예제코드이다.</summary></entry></feed>