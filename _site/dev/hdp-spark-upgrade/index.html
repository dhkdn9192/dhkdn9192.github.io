<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.20.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>HDP 2.6에서 Spark 업그레이드하기 | 오류동 개발자</title>
<meta name="description" content="1. Spark 2.2의 import 버그 현재 사용 중인 하둡 클러스터는 HDP 2.6이며 Spark은 2.2 버전을 제공하고 있다. 해당 버전은 spark-shell에서 정상적으로 import가 작동하지 않는 버그(SPARK-22393)가 존재하므로 Zeppelin에서 제대로 Spark 코드를 수행할 수가 없었다. 따라서 해당 버그가 수정된 버전인 Spark 2.4 버전을 HDP 2.6에 설치해 보고자 한다.">


  <meta name="author" content="김동혁">


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="오류동 개발자">
<meta property="og:title" content="HDP 2.6에서 Spark 업그레이드하기">
<meta property="og:url" content="http://localhost:4000/dev/hdp-spark-upgrade/">


  <meta property="og:description" content="1. Spark 2.2의 import 버그 현재 사용 중인 하둡 클러스터는 HDP 2.6이며 Spark은 2.2 버전을 제공하고 있다. 해당 버전은 spark-shell에서 정상적으로 import가 작동하지 않는 버그(SPARK-22393)가 존재하므로 Zeppelin에서 제대로 Spark 코드를 수행할 수가 없었다. 따라서 해당 버그가 수정된 버전인 Spark 2.4 버전을 HDP 2.6에 설치해 보고자 한다.">







  <meta property="article:published_time" content="2020-09-08T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/dev/hdp-spark-upgrade/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "김동혁",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="오류동 개발자 Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          오류동 개발자
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categories/">카테고리</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">태그</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/author/dhkim_photo.png" alt="김동혁" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">김동혁</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>데이터 엔지니어 / 데이터 분석가 / 개발자</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Seoul, Republic of Korea</span>
        </li>
      

      
        
          
        
          
            <li><a href="https://dhkdn9192.github.io" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
        
          
        
          
            <li><a href="https://github.com/dhkdn9192" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
      

      

      
        <li>
          <a href="mailto:dhkdn9192@naver.com">
            <meta itemprop="email" content="dhkdn9192@naver.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">이메일</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="HDP 2.6에서 Spark 업그레이드하기">
    <meta itemprop="description" content="1. Spark 2.2의 import 버그현재 사용 중인 하둡 클러스터는 HDP 2.6이며 Spark은 2.2 버전을 제공하고 있다.해당 버전은 spark-shell에서 정상적으로 import가 작동하지 않는 버그(SPARK-22393)가 존재하므로 Zeppelin에서 제대로 Spark 코드를 수행할 수가 없었다.따라서 해당 버그가 수정된 버전인 Spark 2.4 버전을 HDP 2.6에 설치해 보고자 한다.">
    <meta itemprop="datePublished" content="2020-09-08T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">HDP 2.6에서 Spark 업그레이드하기
</h1>
          


  <p class="page__meta">

    

    

    
      
      

      <i class="far fa-clock" aria-hidden="true"></i>
      
        4 분 소요
      
    

  </p>

        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> 목차</h4></header>
              <ul class="toc__menu">
  <li><a href="#1-spark-22의-import-버그">1. Spark 2.2의 import 버그</a></li>
  <li><a href="#2-hdp에-외부-spark-연동하기-yarn-client">2. HDP에 외부 Spark 연동하기 (yarn-client)</a>
    <ul>
      <li><a href="#2-1-spark-246-다운로드-및-설치">2-1. Spark 2.4.6 다운로드 및 설치</a></li>
      <li><a href="#2-2-spark-envsh-및-spark_home-설정">2-2. spark-env.sh 및 SPARK_HOME 설정</a></li>
      <li><a href="#2-3-spark-shell-실행">2-3. spark-shell 실행</a></li>
      <li><a href="#2-4-yarn-timeline-service-이슈">2-4. YARN timeline-service 이슈</a></li>
      <li><a href="#2-5-yarn-application-has-already-ended-hdpversion-이슈">2-5. YARN application has already ended! (hdp.version 이슈)</a></li>
    </ul>
  </li>
  <li><a href="#3-hdp에-spark-cluster-설치하기-yarn-cluster">3. HDP에 Spark Cluster 설치하기 (yarn-cluster)</a>
    <ul>
      <li><a href="#3-1-yarn-client-vs-yarn-cluster">3-1. yarn-client vs. yarn-cluster</a></li>
      <li><a href="#3-2-ssh-연결-설정">3-2. ssh 연결 설정</a></li>
      <li><a href="#3-3-spark-cluster를-위한-설정">3-3. Spark Cluster를 위한 설정</a>
        <ul>
          <li><a href="#java-opts">java-opts</a></li>
          <li><a href="#spark-envsh">spark-env.sh</a></li>
          <li><a href="#slaves">slaves</a></li>
        </ul>
      </li>
      <li><a href="#3-4-spark-cluster-설치">3-4. Spark Cluster 설치</a></li>
      <li><a href="#3-5-spark-cluster-실행">3-5. Spark Cluster 실행</a></li>
    </ul>
  </li>
</ul>

            </nav>
          </aside>
        
        <h2 id="1-spark-22의-import-버그">1. Spark 2.2의 import 버그</h2>
<p>현재 사용 중인 하둡 클러스터는 HDP 2.6이며 Spark은 2.2 버전을 제공하고 있다.
해당 버전은 spark-shell에서 정상적으로 import가 작동하지 않는 버그(<a href="https://issues.apache.org/jira/browse/SPARK-22393">SPARK-22393</a>)가 존재하므로 Zeppelin에서 제대로 Spark 코드를 수행할 수가 없었다.
따라서 해당 버그가 수정된 버전인 Spark 2.4 버전을 HDP 2.6에 설치해 보고자 한다.</p>

<h2 id="2-hdp에-외부-spark-연동하기-yarn-client">2. HDP에 외부 Spark 연동하기 (yarn-client)</h2>
<h3 id="2-1-spark-246-다운로드-및-설치">2-1. Spark 2.4.6 다운로드 및 설치</h3>

<p>우선 Apache Ambari 관리모드에서 기존 Spark을 삭제한 뒤, Spark 신규 버전을 Master 노드에 다운로드하였다.
HDP 2.6에선 Hadoop 2.7을 사용하므로 <strong>Pre-built for Apache Hadoop 2.7</strong>을 선택한다.</p>

<p><img src="https://raw.githubusercontent.com/dhkdn9192/dhkdn9192.github.io/master/assets/images/posts/2020/09/08/2020-09-08-download-spark.png" alt="download-spark" /></p>

<h3 id="2-2-spark-envsh-및-spark_home-설정">2-2. spark-env.sh 및 SPARK_HOME 설정</h3>
<p>원하는 곳에 압축 해제한 뒤 <code class="language-plaintext highlighter-rouge">conf/spark-env.sh</code> 파일을 열어 아래와 같이 수정한다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/usr/hdp/2.6.3.0-235/hadoop
<span class="nb">export </span><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span>/etc/hadoop/conf
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">HADOOP_HOME</code>은 HDP Hadoop이 설치된 경로이다.
Spark은 <code class="language-plaintext highlighter-rouge">HADOOP_CONF_DIR}</code> 하위의 <code class="language-plaintext highlighter-rouge">yarn-site.xml</code> 파일을 이용해 YARN과 연결할 수 있게 된다.</p>

<p>또한, <code class="language-plaintext highlighter-rouge">~/.bashrc</code> 파일을 열어 아래와 같이 <code class="language-plaintext highlighter-rouge">SPARK_HOME</code>, <code class="language-plaintext highlighter-rouge">PATH</code> 환경변수를 설정해준다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">={</span>설치한 Spark 디렉토리 경로<span class="o">}</span>
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$SPARK_HOME</span>/bin:<span class="nv">$PATH</span>
</code></pre></div></div>

<h3 id="2-3-spark-shell-실행">2-3. spark-shell 실행</h3>

<p><code class="language-plaintext highlighter-rouge">bin/spark-shell</code>을 실행하면 REPL 환경에서 Spark 프로그래밍을 할 수 있다.
별도의 옵션 없이 수행하면 standalone 모드로 동작한다. YARN과 연동하여 하둡 클러스터를 이용하려면 yarn-client 모드로 동작해야 한다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./bin/spark-shell <span class="nt">--master</span> yarn <span class="nt">--deploy-mode</span> client
</code></pre></div></div>

<h3 id="2-4-yarn-timeline-service-이슈">2-4. YARN timeline-service 이슈</h3>

<p>hdp에서 위와 같이 yarn-client 모드를 수행하면 다음과 같은 에러가  발생한다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig
  at org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient(TimelineClient.java:55)
  at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createTimelineClient(YarnClientImpl.java:181)
  at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:168)
  at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
  at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:161)
  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:188)
  at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:501)
  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
  at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
  ... 62 elided
Caused by: java.lang.ClassNotFoundException: com.sun.jersey.api.client.config.ClientConfig
  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 76 more
</code></pre></div></div>

<p>다행히도 이 오류는 <a href="https://issues.apache.org/jira/browse/SPARK-15343">SPARK-15343</a>에서 다뤄진 적이 있다.
이슈 내용을 정라하자면 핵심은 다음과 같다.</p>

<ul>
  <li>YARN의 timeline-service 기능과 관련하여 jersey 라이브러리를 사용하고 있다.</li>
  <li>기존에 사용하던 HDP 환경에선 YARN과 Spark 모두 jersey 1.9 버전을 사용하고 있었으나, <strong>Spark 2.4는 2.22.2 버전을 사용하므로 서로 호환되지 않는다</strong>.</li>
  <li>YARN은 디폴트로 timeline-service 기능이 켜져있으므로 <strong>항상 jersey 호환성으로 인한 NoClassDefFoundError를 일으키게 된다</strong>.</li>
</ul>

<p>jira 이슈에서 제시되는 몇가지 해결책들이 있다.</p>

<ul>
  <li>timeline-service를 중지하기 위해 YARN 설정에서 <code class="language-plaintext highlighter-rouge">yarn.timeline-service.enabled</code> 옵션을 false로 변경하거나</li>
  <li>spark-shell을 수행할 때 <code class="language-plaintext highlighter-rouge">--conf spark.timeline-service.enabled=false</code> 옵션을 추가해주거나</li>
  <li>spark의 jars 디렉토리에 jersey 1.9 라이브러리를 넣어서 버전을 통일시켜주는 방법이 있다.</li>
</ul>

<p>YARN에 제출되는 다양한 애플리케이션들 중 오직 Spark만을 위해서 timeline-service 기능을 끄는 것은 다소 실용성이 떨어진다.
jersey의 버전을 강제적으로 통일했을 땐 부가적인 문제가 생길 가능성이 있으므로 여기서는 두번째 방안을 채택하기로 했다.
옵션을 디폴트로 설정하려면 <code class="language-plaintext highlighter-rouge">conf/spark-defaults.conf</code> 파일에 아래와 같이 추가해준다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark.hadoop.yarn.timeline-service.enabled   false
</code></pre></div></div>

<h3 id="2-5-yarn-application-has-already-ended-hdpversion-이슈">2-5. YARN application has already ended! (hdp.version 이슈)</h3>

<p>timeline-service 이슈를 해결했더니 이번에는 다음과 같은 에러가 발생했다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>20/09/08 19:42:53 ERROR cluster.YarnClientSchedulerBackend: The YARN application has already ended! Itmight have been killed or the Application Master may have failed to start. Check the YARN application logs for more details.
20/09/08 19:42:53 ERROR spark.SparkContext: Error initializing SparkContext.
org.apache.spark.SparkException: Application application_1599556837475_0006 failed 2 times due to AM Container for appattempt_1599556837475_0006_000002 exited with  exitCode: 1
For more detailed output, check the application tracking page: http://불라불라불라/cluster/app/application_1599556837475_0006 Then click on links to logs of each attempt.
Diagnostics: Exception from container-launch.
Container id: container_e16_1599556837475_0006_02_000001
Exit code: 1
</code></pre></div></div>

<p>YARN에 작업이 제출된 즉시 애플리케이션이 종료된다.
원인을 찾느라 한참 헤맸는데 사실은 정말 단순한 이유였다.
HDP에선 Spark job을 제출할 때 java 옵션으로 HDP 버전을 명시해줘야 하는데,
HDP와는 별개로 설치한 Spark 패키지여서 이 부분을 생략했던 것이다.</p>

<p><a href="https://stackoverflow.com/questions/55931970/how-can-i-run-spark-in-headless-mode-in-my-custom-version-on-hdp">스택오버플로우</a>에 올라온 답변을 참조하여 다음과 같이 옵션을 추가하였다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./bin/spark-shell <span class="se">\</span>
  <span class="nt">--master</span> yarn <span class="se">\</span>
  <span class="nt">--deploy-mode</span> client <span class="se">\</span>
  <span class="nt">--conf</span> spark.hadoop.yarn.timeline-service.enabled<span class="o">=</span><span class="nb">false</span> <span class="se">\</span>
  <span class="nt">--conf</span> spark.driver.extraJavaOptions<span class="o">=</span><span class="s1">'-Dhdp.version=2.6.3.0-235'</span> <span class="se">\</span>
  <span class="nt">--conf</span> spark.yarn.am.extraJavaOptions<span class="o">=</span><span class="s1">'-Dhdp.version=2.6.3.0-235'</span> <span class="se">\</span>
  <span class="nt">--conf</span> spark.executor.extraJavaOptions<span class="o">=</span><span class="s1">'-Dhdp.version=2.6.3.0-235'</span>
</code></pre></div></div>

<p>여기까지 수행한 끝에 HDP 2.6에서 yarn-client 모드로 Spark 2.4 버전을 사용할 수 있게 되었다.</p>

<p><img src="https://raw.githubusercontent.com/dhkdn9192/dhkdn9192.github.io/master/assets/images/posts/2020/09/08/2020-09-08-spark-shell-result.jpeg" alt="result" /></p>

<p>해당 옵션을 디폴트로 사용하려면 <code class="language-plaintext highlighter-rouge">conf/spark-defaults.conf</code>에 아래 라인들을 추가해준다.
참고로 <code class="language-plaintext highlighter-rouge">spark.yarn.am.extraJavaOptions</code> 옵션은 yarn-cluster 모드에선 효과가 없다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark.driver.extraJavaOptions    -Dhdp.version=2.6.3.0-235
spark.yarn.am.extraJavaOptions   -Dhdp.version=2.6.3.0-235
spark.executor.extraJavaOptions  -Dhdp.version=2.6.3.0-235
</code></pre></div></div>

<h2 id="3-hdp에-spark-cluster-설치하기-yarn-cluster">3. HDP에 Spark Cluster 설치하기 (yarn-cluster)</h2>
<h3 id="3-1-yarn-client-vs-yarn-cluster">3-1. yarn-client vs. yarn-cluster</h3>

<p>위처럼 yarn-client 모드로 사용할 땐 master 노드에만 spark을 설치하면 된다.
그러나 yarn-cluster 모드를 사용하려면 slave 노드들에도 spark이 설치되어야 한다.</p>

<ul>
  <li>yarn-client</li>
</ul>

<p><img src="https://raw.githubusercontent.com/dhkdn9192/dhkdn9192.github.io/master/assets/images/posts/2020/09/11/2020-09-11-yarn-client-mode.png" alt="yarn-client" /></p>

<ul>
  <li>yarn-cluster</li>
</ul>

<p><img src="https://raw.githubusercontent.com/dhkdn9192/dhkdn9192.github.io/master/assets/images/posts/2020/09/11/2020-09-11-yarn-cluster-mode.png" alt="yarn-cluster" /></p>

<ul>
  <li>이미지 출처 : https://medium.com/@goyalsaurabh66/running-spark-jobs-on-yarn-809163fc57e2</li>
</ul>

<p>위 이미지에서 보듯이, client 모드와 cluster 모드의 가장 큰 차이는 spark driver의 위치다.
cluster 모드에선 spark driver가 application master에서 동작한다.
즉 클러스터 내부에서 spark driver가 생성되어야 한다.
모든 slave 노드들에 spark을 설치해줘야 하는 이유다.</p>

<h3 id="3-2-ssh-연결-설정">3-2. ssh 연결 설정</h3>

<p>spark cluster가 구성되려면 각 노드의 spark 계정끼리 ssh로 연결 가능해햐 한다.
ssh 키가 없다면 아래 명령어로 생성해준다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ssh-keygen
</code></pre></div></div>

<p>master 노드의 ssh public 키를 나머지 노드들에 복사해준다.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ssh-copy-id <span class="o">{</span>계정<span class="o">}</span>@<span class="o">{</span>서버IP<span class="o">}</span> <span class="nt">-p</span> <span class="o">{</span>ssh포트번호<span class="o">}</span>
</code></pre></div></div>

<p>그 후, 각 slave 장비에 접속하여 <code class="language-plaintext highlighter-rouge">~/.ssh/authorized_keys</code> 파일의 권한을 확인해준다.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">chmod </span>600 ~/.ssh/authorized_keys
</code></pre></div></div>

<h3 id="3-3-spark-cluster를-위한-설정">3-3. Spark Cluster를 위한 설정</h3>

<p>slave 노드들에 spark을 설치해주기 전에 master 노드의 spark에 몇가지 설정을 변경해주어야 한다.</p>

<h4 id="java-opts">java-opts</h4>
<p><code class="language-plaintext highlighter-rouge">conf/java-opts</code> 파일을 생성하고 아래 라인을 추가한다.
yarn-cluster 모드에서도 hdp.version을 맞추기 위한 설정파일이다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-Dhdp.version=2.6.3.0-235
</code></pre></div></div>

<h4 id="spark-envsh">spark-env.sh</h4>

<p><code class="language-plaintext highlighter-rouge">conf/spark-env.sh</code> 파일에 다음 라인들을 추가한다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">SPARK_MASTER</span><span class="o">=</span><span class="s1">'{master 서버 IP}'</span>
<span class="nb">export </span><span class="nv">SPARK_WORKER_PORT</span><span class="o">={</span>ssh포트번호<span class="o">}</span>
<span class="nb">export </span><span class="nv">SPARK_SSH_OPTS</span><span class="o">=</span><span class="s1">'-p {ssh포트번호}'</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">={</span>JAVA_HOME<span class="o">}</span>
</code></pre></div></div>

<h4 id="slaves">slaves</h4>

<p><code class="language-plaintext highlighter-rouge">conf/slaves.template</code> 파일 이름을 <code class="language-plaintext highlighter-rouge">conf/slaves</code>로 변경하고 아래 라인들을 추가한다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{slave1 서버IP}
{slave2 서버IP}
{slave3 서버IP}
...(하략)...
</code></pre></div></div>

<h3 id="3-4-spark-cluster-설치">3-4. Spark Cluster 설치</h3>

<p>이제 master 노드의 spark을 압축하여 모든 slave 노드들의 동일한 경로에 복사/압축해제해준다.
각 노드의 <code class="language-plaintext highlighter-rouge">~/.bashrc</code>파일에 master 노드와 동일한 설정을 추가해준다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">={</span>설치한 Spark 디렉토리 경로<span class="o">}</span>
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$SPARK_HOME</span>/bin:<span class="nv">$PATH</span>
</code></pre></div></div>

<p>끝으로 <code class="language-plaintext highlighter-rouge">JAVA_HOME</code> 역시 확인해준다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">echo</span> <span class="nv">$JAVA_HOME</span>
</code></pre></div></div>

<h3 id="3-5-spark-cluster-실행">3-5. Spark Cluster 실행</h3>

<p>master 노드의 <code class="language-plaintext highlighter-rouge">SPARK_HOME</code>에서 다음 명령어로 spark cluster를 실행해준다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>./sbin/start-all.sh
</code></pre></div></div>

<p>기존에 개발된 spark 프로그램이 yarn-cluster 모드에서 정상 동작하는 것까지 확인하였다.
여기까지 수행하면 Spark 2.4.6 설치가 완료된 것이다.</p>

<p>웬만하면 Ambari나 Cloudera Manager가 제공해주는 버전을 그냥 쓰는게 정신건강에 이로울 것 같다.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#hdp" class="page__taxonomy-item" rel="tag">hdp</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#scala" class="page__taxonomy-item" rel="tag">scala</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#spark" class="page__taxonomy-item" rel="tag">spark</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#yarn" class="page__taxonomy-item" rel="tag">yarn</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#dev" class="page__taxonomy-item" rel="tag">dev</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2020-09-08T00:00:00+09:00">September 8, 2020</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=HDP+2.6%EC%97%90%EC%84%9C+Spark+%EC%97%85%EA%B7%B8%EB%A0%88%EC%9D%B4%EB%93%9C%ED%95%98%EA%B8%B0%20http%3A%2F%2Flocalhost%3A4000%2Fdev%2Fhdp-spark-upgrade%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fdev%2Fhdp-spark-upgrade%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fdev%2Fhdp-spark-upgrade%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/dev/sparkshell-import-error/" class="pagination--pager" title="Spark Shell import 에러
">이전</a>
    
    
      <a href="/dev/pyspark-py4j-error/" class="pagination--pager" title="PySpark의 py4j 호환성 오류
">다음</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">참고</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dev/pyspark-py4j-error/" rel="permalink">PySpark의 py4j 호환성 오류
</a>
      
    </h2>
    


  <p class="page__meta">

    

    

    
      
      

      <i class="far fa-clock" aria-hidden="true"></i>
      
        1 분 소요
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">1. Py4JError
기존에 사용하던 Spark을 2.2.0 -&gt; 2.4.6으로 변경했더니 Jupyter에서 PySpark이 정상적으로 동작하지 않는 오류가 발생했다.
실행한 코드는 아래와 같이 SparkSession을 생성하는 간단한 내용이다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dev/sparkshell-import-error/" rel="permalink">Spark Shell import 에러
</a>
      
    </h2>
    


  <p class="page__meta">

    

    

    
      
      

      <i class="far fa-clock" aria-hidden="true"></i>
      
        1 분 소요
      
    

  </p>

    <p class="archive__item-excerpt" itemprop="description">1. 문제 발견
Zeppelin에서 Spark을 사용하다가 이해할 수 없는 에러를 보게 되었다.
아래는 “9가지 사례로 익히는 고급 스파크 분석” 책의 예제코드이다.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
      
        
      
        
          <li><a href="https://github.com/dhkdn9192" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
        
          <li><a href="mailto:dhkdn9192@naver.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 김동혁. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/dev/hdp-spark-upgrade/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/dev/hdp-spark-upgrade"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://dhkdn9192.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
